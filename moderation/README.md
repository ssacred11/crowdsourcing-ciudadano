# Moderation Module

MÃ³dulo de **moderaciÃ³n de incidencias** para *crowdsourcing-ciudadano*. Detecta lenguaje inapropiado, amenazas, violencia o spam antes de que el reporte entre al flujo normal.

---

## âœ¨ Firma y salida

```python
from moderation.core.analyzer import analyze_report

res = analyze_report("ese imbÃ©cil rompiÃ³ la ventana")
print(res.flagged, res.reasons, res.severity)
# True ['match:\b(...)\b [insulto+2]'] 'medium'
```

**Firma real:**
```python
analyze_report(text: str) -> ModResult(
  flagged: bool,
  reasons: list[str],
  severity: Literal["low","medium","high"]
)
```

> Nota para backend: si quieres severidad en espaÃ±ol, mapea asÃ­:
> ```python
> SEV_ES = {"low":"leve", "medium":"media", "high":"grave"}
> es = SEV_ES[res.severity]
> ```

---

## ðŸ“‚ Estructura

```
moderation/
â”œâ”€ core/
â”‚  â”œâ”€ analyzer.py        # LÃ³gica principal
â”‚  â””â”€ patterns_es.txt    # Patrones regex categorizados
â”œâ”€ tests/
â”‚  â””â”€ test_analyzer.py   # Pruebas unitarias (pytest)
â””â”€ README.md
```

---

## ðŸ§ª Ejecutar tests

```bash
# Windows PowerShell
.\.venv\Scripts\Activate.ps1
pytest moderation/
```

Todos los tests deben pasar âœ….

---

## ðŸ§© Patrones y categorÃ­as

Patrones en `moderation/core/patterns_es.txt`.  
Cada lÃ­nea es un regex con categorÃ­a al inicio entre llaves:

```text
{insulto}\b(weon|weÃ³n|qlo|culiao|idiota|imbÃ©cil|estÃºpido)\b
{amenaza}\b(te (voy|iremos) a (pegar|golpear|romper|reventar))\b
{violencia}\b(arma|cuchillo|pistola|golp(e|ear)|pelea|agresiÃ³n)\b
{contenido_sensible}\b(acoso|bullying|hostigar|amenazar)\b
{spam}gratis\s*100%
```

Las categorÃ­as tienen pesos heurÃ­sticos y generan una **severity**:
- `low` | `medium` | `high`
- Modo estricto (mÃ¡s sensible): `MOD_STRICT=true`

---

## ðŸ”Œ IntegraciÃ³n en backend (FastAPI)

En `POST /incidents`, unir `title + description`, llamar al analizador y guardar campos:

```python
from moderation.core.analyzer import analyze_report

payload_text = f"{title} {description}".strip()
res = analyze_report(payload_text)

incident.moderation_flag = res.flagged
incident.moderation_reasons = res.reasons
# opcional: severidad en espaÃ±ol
SEV_ES = {"low":"leve", "medium":"media", "high":"grave"}
incident.moderation_severity = SEV_ES[res.severity]
```

**Contrato sugerido (OpenAPI v0.1.1):**
```json
{
  "moderation_flag": true,
  "moderation_reasons": ["insulto", "amenaza"],
  "severity": "media"
}
```

Fail-open recomendado: si el mÃ³dulo falla, crear igualmente la incidencia con `moderation_flag=false`.

---

## ðŸ”„ Calidad y CI (sugerido)
- GitHub Actions: job que corra `pytest moderation/` en cada PR.
- `pre-commit` (black/ruff) y `CODEOWNERS` para asignar `/moderation` al owner del mÃ³dulo.

---
